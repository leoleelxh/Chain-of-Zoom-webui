#!/usr/bin/env python3
"""
GPUå†…å­˜ä¼˜åŒ–å·¥å…·
ç”¨äºChain-of-Zoomé¡¹ç›®çš„å†…å­˜ç®¡ç†å’Œä¼˜åŒ–
"""

import torch
import gc
import os
import psutil
import subprocess
import time

class MemoryOptimizer:
    def __init__(self):
        """åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–å™¨"""
        self.device_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
    def get_gpu_memory_info(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if not torch.cuda.is_available():
            return "CUDAä¸å¯ç”¨"
        
        info = []
        for i in range(self.device_count):
            props = torch.cuda.get_device_properties(i)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            reserved = torch.cuda.memory_reserved(i) / 1024**3
            total = props.total_memory / 1024**3
            
            info.append({
                'device': i,
                'name': props.name,
                'total': total,
                'allocated': allocated,
                'reserved': reserved,
                'free': total - reserved
            })
        return info
    
    def print_memory_status(self):
        """æ‰“å°å†…å­˜çŠ¶æ€"""
        print("=" * 60)
        print("ğŸ” GPUå†…å­˜çŠ¶æ€")
        print("=" * 60)
        
        gpu_info = self.get_gpu_memory_info()
        if isinstance(gpu_info, str):
            print(gpu_info)
            return
            
        for info in gpu_info:
            print(f"GPU {info['device']}: {info['name']}")
            print(f"  æ€»å†…å­˜: {info['total']:.1f}GB")
            print(f"  å·²åˆ†é…: {info['allocated']:.1f}GB ({info['allocated']/info['total']*100:.1f}%)")
            print(f"  å·²ä¿ç•™: {info['reserved']:.1f}GB ({info['reserved']/info['total']*100:.1f}%)")
            print(f"  å¯ç”¨: {info['free']:.1f}GB ({info['free']/info['total']*100:.1f}%)")
            print()
        
        # ç³»ç»Ÿå†…å­˜
        ram = psutil.virtual_memory()
        print(f"ç³»ç»Ÿå†…å­˜: {ram.used/1024**3:.1f}GB / {ram.total/1024**3:.1f}GB ({ram.percent:.1f}%)")
        print("=" * 60)
    
    def clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        print("ğŸ§¹ æ¸…ç†GPUå†…å­˜...")
        
        if torch.cuda.is_available():
            # æ¸…ç†PyTorchç¼“å­˜
            torch.cuda.empty_cache()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # åŒæ­¥æ‰€æœ‰GPU
            for i in range(self.device_count):
                torch.cuda.synchronize(i)
            
            print("âœ… GPUå†…å­˜æ¸…ç†å®Œæˆ")
        else:
            print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•æ¸…ç†GPUå†…å­˜")
    
    def optimize_for_coz(self):
        """ä¸ºChain-of-Zoomä¼˜åŒ–ç³»ç»Ÿ"""
        print("ğŸš€ ä¸ºChain-of-Zoomä¼˜åŒ–ç³»ç»Ÿ...")
        
        # æ¸…ç†å†…å­˜
        self.clear_gpu_memory()
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥æ‰§è¡Œï¼Œä¾¿äºè°ƒè¯•
        
        # è®¾ç½®PyTorchå†…å­˜åˆ†é…ç­–ç•¥
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = False  # ç¦ç”¨cudnn benchmarkä»¥èŠ‚çœå†…å­˜
            torch.backends.cudnn.deterministic = True
        
        print("âœ… ç³»ç»Ÿä¼˜åŒ–å®Œæˆ")
    
    def monitor_memory_usage(self, duration=60, interval=5):
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ ({duration}ç§’ï¼Œæ¯{interval}ç§’æ›´æ–°)")
        print("æŒ‰Ctrl+Cåœæ­¢ç›‘æ§")
        
        try:
            start_time = time.time()
            while time.time() - start_time < duration:
                os.system('clear' if os.name == 'posix' else 'cls')
                self.print_memory_status()
                print(f"ç›‘æ§æ—¶é—´: {time.time() - start_time:.1f}s / {duration}s")
                time.sleep(interval)
        except KeyboardInterrupt:
            print("\nç›‘æ§å·²åœæ­¢")
    
    def get_recommended_settings(self):
        """æ ¹æ®GPUå†…å­˜æ¨èè®¾ç½®"""
        if not torch.cuda.is_available():
            return "CUDAä¸å¯ç”¨ï¼Œæ— æ³•æä¾›æ¨èè®¾ç½®"
        
        gpu_info = self.get_gpu_memory_info()
        main_gpu = gpu_info[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUçš„ä¿¡æ¯
        total_memory = main_gpu['total']
        
        print("ğŸ¯ æ¨èè®¾ç½®ï¼ˆåŸºäºä½ çš„GPUå†…å­˜ï¼‰:")
        print("=" * 50)
        
        if total_memory >= 20:  # 20GB+
            print("ä½ çš„GPUå†…å­˜å……è¶³ (â‰¥20GB)ï¼Œæ¨èè®¾ç½®:")
            print("- Process Size: 512-768")
            print("- VAE Tiled Size: 256-384")
            print("- Latent Tiled Size: 96-128")
            print("- Recursion Number: 4-6")
            print("- Efficient Memory: å¯é€‰")
            print("- Mixed Precision: fp16")
        elif total_memory >= 12:  # 12-20GB
            print("ä½ çš„GPUå†…å­˜ä¸­ç­‰ (12-20GB)ï¼Œæ¨èè®¾ç½®:")
            print("- Process Size: 384-512")
            print("- VAE Tiled Size: 192-256")
            print("- Latent Tiled Size: 80-96")
            print("- Recursion Number: 3-4")
            print("- Efficient Memory: å»ºè®®å¼€å¯")
            print("- Mixed Precision: fp16")
        elif total_memory >= 8:   # 8-12GB
            print("ä½ çš„GPUå†…å­˜è¾ƒå°‘ (8-12GB)ï¼Œæ¨èè®¾ç½®:")
            print("- Process Size: 256-384")
            print("- VAE Tiled Size: 128-192")
            print("- Latent Tiled Size: 64-80")
            print("- Recursion Number: 2-3")
            print("- Efficient Memory: å¿…é¡»å¼€å¯")
            print("- Mixed Precision: fp16")
        else:  # <8GB
            print("ä½ çš„GPUå†…å­˜ä¸è¶³ (<8GB)ï¼Œæ¨èè®¾ç½®:")
            print("- Process Size: 256")
            print("- VAE Tiled Size: 128")
            print("- Latent Tiled Size: 64")
            print("- Recursion Number: 2")
            print("- Efficient Memory: å¿…é¡»å¼€å¯")
            print("- Mixed Precision: fp16")
            print("âš ï¸  è­¦å‘Š: å†…å­˜å¯èƒ½ä»ç„¶ä¸è¶³ï¼Œå»ºè®®å‡çº§GPU")
        
        print("=" * 50)
    
    def kill_gpu_processes(self):
        """ç»ˆæ­¢å ç”¨GPUçš„è¿›ç¨‹ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
        print("âš ï¸  æŸ¥æ‰¾å ç”¨GPUçš„è¿›ç¨‹...")
        
        try:
            # ä½¿ç”¨nvidia-smiæŸ¥æ‰¾GPUè¿›ç¨‹
            result = subprocess.run(['nvidia-smi', '--query-compute-apps=pid,process_name,used_memory', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                print("å‘ç°ä»¥ä¸‹GPUè¿›ç¨‹:")
                for line in lines:
                    parts = line.split(', ')
                    if len(parts) >= 3:
                        pid, name, memory = parts[0], parts[1], parts[2]
                        print(f"  PID: {pid}, è¿›ç¨‹: {name}, å†…å­˜: {memory}MB")
                
                response = input("\næ˜¯å¦è¦ç»ˆæ­¢è¿™äº›è¿›ç¨‹? (y/N): ")
                if response.lower() == 'y':
                    for line in lines:
                        pid = line.split(', ')[0]
                        try:
                            os.kill(int(pid), 9)
                            print(f"âœ… å·²ç»ˆæ­¢è¿›ç¨‹ {pid}")
                        except:
                            print(f"âŒ æ— æ³•ç»ˆæ­¢è¿›ç¨‹ {pid}")
                else:
                    print("å–æ¶ˆæ“ä½œ")
            else:
                print("âœ… æ²¡æœ‰å‘ç°å ç”¨GPUçš„è¿›ç¨‹")
                
        except FileNotFoundError:
            print("âŒ nvidia-smiæœªæ‰¾åˆ°ï¼Œæ— æ³•æŸ¥è¯¢GPUè¿›ç¨‹")
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    optimizer = MemoryOptimizer()
    
    print("ğŸ”§ Chain-of-Zoom GPUå†…å­˜ä¼˜åŒ–å·¥å…·")
    print("=" * 50)
    
    while True:
        print("\né€‰æ‹©æ“ä½œ:")
        print("1. æŸ¥çœ‹å†…å­˜çŠ¶æ€")
        print("2. æ¸…ç†GPUå†…å­˜")
        print("3. ä¼˜åŒ–ç³»ç»Ÿè®¾ç½®")
        print("4. è·å–æ¨èè®¾ç½®")
        print("5. ç›‘æ§å†…å­˜ä½¿ç”¨")
        print("6. ç»ˆæ­¢GPUè¿›ç¨‹ (è°¨æ…)")
        print("0. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-6): ").strip()
        
        if choice == '1':
            optimizer.print_memory_status()
        elif choice == '2':
            optimizer.clear_gpu_memory()
            optimizer.print_memory_status()
        elif choice == '3':
            optimizer.optimize_for_coz()
        elif choice == '4':
            optimizer.get_recommended_settings()
        elif choice == '5':
            duration = input("ç›‘æ§æ—¶é•¿(ç§’ï¼Œé»˜è®¤60): ").strip()
            duration = int(duration) if duration.isdigit() else 60
            optimizer.monitor_memory_usage(duration)
        elif choice == '6':
            optimizer.kill_gpu_processes()
        elif choice == '0':
            print("ğŸ‘‹ å†è§!")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")

if __name__ == "__main__":
    main() 